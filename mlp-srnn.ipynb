{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xQS1bVbsHPsn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data_dir = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "XRVxyeeQHYyr",
        "outputId": "736094f5-e0c0-462a-8503-f1244c92d5b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading CIFAR-10 dataset ...\n",
            "Elapsed time to read dataset: 1.524495 sec\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'asscalar'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m cifar10 \u001b[39m=\u001b[39m CIFAR10Data(data_dir, use_one_hot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, validation_size\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m cifar_inps, cifar_trgts \u001b[39m=\u001b[39m cifar10\u001b[39m.\u001b[39mnext_train_batch(\u001b[39m4\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m cifar10\u001b[39m.\u001b[39;49mplot_samples(\u001b[39m'\u001b[39;49m\u001b[39mCIFAR10 Examples\u001b[39;49m\u001b[39m'\u001b[39;49m, cifar_inps, outputs\u001b[39m=\u001b[39;49mcifar_trgts)\n",
            "File \u001b[0;32m~/Desktop/hypernetwork-continual/.venv/lib/python3.9/site-packages/hypnettorch/data/dataset.py:880\u001b[0m, in \u001b[0;36mDataset.plot_samples\u001b[0;34m(self, title, inputs, outputs, predictions, num_samples_per_row, show, filename, interactive, figsize, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[39mif\u001b[39;00m predictions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    878\u001b[0m         preds \u001b[39m=\u001b[39m predictions[i, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[0;32m--> 880\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_sample(fig, inner_grid, pc[\u001b[39m'\u001b[39;49m\u001b[39mnum_inner_plots\u001b[39;49m\u001b[39m'\u001b[39;49m], i,\n\u001b[1;32m    881\u001b[0m                       inputs[i, np\u001b[39m.\u001b[39;49mnewaxis], outputs\u001b[39m=\u001b[39;49mouts, \n\u001b[1;32m    882\u001b[0m                       predictions\u001b[39m=\u001b[39;49mpreds, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    884\u001b[0m \u001b[39mif\u001b[39;00m show:\n\u001b[1;32m    885\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n",
            "File \u001b[0;32m~/Desktop/hypernetwork-continual/.venv/lib/python3.9/site-packages/hypnettorch/data/cifar10_data.py:393\u001b[0m, in \u001b[0;36mCIFAR10Data._plot_sample\u001b[0;34m(self, fig, inner_grid, num_inner_plots, ind, inputs, outputs, predictions)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[39massert\u001b[39;00m(np\u001b[39m.\u001b[39msize(outputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m     label \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masscalar(outputs)\n\u001b[1;32m    394\u001b[0m     label_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39m'\u001b[39m\u001b[39mcifar10\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabel_names\u001b[39m\u001b[39m'\u001b[39m][label]\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m predictions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/hypernetwork-continual/.venv/lib/python3.9/site-packages/numpy/__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m Tester\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 311\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'asscalar'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from hypnettorch.data import CIFAR10Data\n",
        "cifar10 = CIFAR10Data(data_dir, use_one_hot=True, validation_size=0)\n",
        "cifar_inps, cifar_trgts = cifar10.next_train_batch(4)\n",
        "cifar10.plot_samples('CIFAR10 Examples', cifar_inps, outputs=cifar_trgts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaoqNOnHHpAK",
        "outputId": "15cc7830-390f-4930-fa5c-7053a13ea446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating a simple RNN with 11380 weights.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SimpleRNN(\n",
              "  (_a_fun): Tanh()\n",
              "  (_weights): ParameterList(\n",
              "      (0): Parameter containing: [torch.float32 of size 40x1]\n",
              "      (1): Parameter containing: [torch.float32 of size 40]\n",
              "      (2): Parameter containing: [torch.float32 of size 40x10]\n",
              "      (3): Parameter containing: [torch.float32 of size 40]\n",
              "      (4): Parameter containing: [torch.float32 of size 80x10]\n",
              "      (5): Parameter containing: [torch.float32 of size 80]\n",
              "      (6): Parameter containing: [torch.float32 of size 80x20]\n",
              "      (7): Parameter containing: [torch.float32 of size 80]\n",
              "      (8): Parameter containing: [torch.float32 of size 120x20]\n",
              "      (9): Parameter containing: [torch.float32 of size 120]\n",
              "      (10): Parameter containing: [torch.float32 of size 120x30]\n",
              "      (11): Parameter containing: [torch.float32 of size 120]\n",
              "      (12): Parameter containing: [torch.float32 of size 50x30]\n",
              "      (13): Parameter containing: [torch.float32 of size 50]\n",
              "      (14): Parameter containing: [torch.float32 of size 10x50]\n",
              "      (15): Parameter containing: [torch.float32 of size 10]\n",
              "  )\n",
              "  (_layer_weight_tensors): ParameterList(\n",
              "      (0): Parameter containing: [torch.float32 of size 40x1]\n",
              "      (1): Parameter containing: [torch.float32 of size 40x10]\n",
              "      (2): Parameter containing: [torch.float32 of size 80x10]\n",
              "      (3): Parameter containing: [torch.float32 of size 80x20]\n",
              "      (4): Parameter containing: [torch.float32 of size 120x20]\n",
              "      (5): Parameter containing: [torch.float32 of size 120x30]\n",
              "      (6): Parameter containing: [torch.float32 of size 50x30]\n",
              "      (7): Parameter containing: [torch.float32 of size 10x50]\n",
              "  )\n",
              "  (_layer_bias_vectors): ParameterList(\n",
              "      (0): Parameter containing: [torch.float32 of size 40]\n",
              "      (1): Parameter containing: [torch.float32 of size 40]\n",
              "      (2): Parameter containing: [torch.float32 of size 80]\n",
              "      (3): Parameter containing: [torch.float32 of size 80]\n",
              "      (4): Parameter containing: [torch.float32 of size 120]\n",
              "      (5): Parameter containing: [torch.float32 of size 120]\n",
              "      (6): Parameter containing: [torch.float32 of size 50]\n",
              "      (7): Parameter containing: [torch.float32 of size 10]\n",
              "  )\n",
              "  (_internal_params): ParameterList(\n",
              "      (0): Parameter containing: [torch.float32 of size 40x1]\n",
              "      (1): Parameter containing: [torch.float32 of size 40]\n",
              "      (2): Parameter containing: [torch.float32 of size 40x10]\n",
              "      (3): Parameter containing: [torch.float32 of size 40]\n",
              "      (4): Parameter containing: [torch.float32 of size 80x10]\n",
              "      (5): Parameter containing: [torch.float32 of size 80]\n",
              "      (6): Parameter containing: [torch.float32 of size 80x20]\n",
              "      (7): Parameter containing: [torch.float32 of size 80]\n",
              "      (8): Parameter containing: [torch.float32 of size 120x20]\n",
              "      (9): Parameter containing: [torch.float32 of size 120]\n",
              "      (10): Parameter containing: [torch.float32 of size 120x30]\n",
              "      (11): Parameter containing: [torch.float32 of size 120]\n",
              "      (12): Parameter containing: [torch.float32 of size 50x30]\n",
              "      (13): Parameter containing: [torch.float32 of size 50]\n",
              "      (14): Parameter containing: [torch.float32 of size 10x50]\n",
              "      (15): Parameter containing: [torch.float32 of size 10]\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#\n",
        "#set the number of input features to 1, \n",
        "##specified 3 RNN layers with 10, 20, and 30 hidden units respectively, \n",
        "#and 2 fully-connected layers with 50 and 1 hidden units \n",
        "\n",
        "from hypnettorch.mnets.simple_rnn import SimpleRNN\n",
        "\n",
        "# Define the number of input features\n",
        "n_in = 1\n",
        "\n",
        "# Define the number of hidden units for each RNN layer\n",
        "rnn_layers = (10, 20, 30)\n",
        "\n",
        "# Define the number of hidden units for each fully-connected layer\n",
        "fc_layers = (50, 10)\n",
        "\n",
        "# Instantiate the SimpleRNN class with the desired parameters\n",
        "mnet = SimpleRNN(n_in=n_in, rnn_layers=rnn_layers, fc_layers=fc_layers, use_lstm=True)\n",
        "\n",
        "# Move the model to the desired device\n",
        "mnet.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fNJM6kYIAnW",
        "outputId": "d0a07dcd-8887-4261-e0e8-7ced22e89534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created MLP Hypernet.\n",
            "Hypernetwork with 1160396 weights and 11380 outputs (compression ratio: 101.97).\n",
            "The network consists of 1160380 unconditional weights (1160380 internally maintained) and 16 conditional weights (16 internally maintained).\n",
            "\n",
            "The randomly initialized input embeddings are:\n",
            " [Parameter containing:\n",
            "tensor([ 0.2845, -1.0180, -1.4538, -0.5949,  1.6931, -0.9578, -2.1631,  0.8994],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.4559,  1.6897, -1.3093,  0.6659, -0.0136,  1.3877, -0.2435,  2.0411],\n",
            "       requires_grad=True)]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Hypernetworks also allow batch processing.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m W_batch \u001b[39m=\u001b[39m hnet\u001b[39m.\u001b[39mforward(cond_id\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[0;32m---> 18\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall([torch\u001b[39m.\u001b[39mequal(W_batch[\u001b[39m0\u001b[39m][i], W_batch[\u001b[39m1\u001b[39m][i]) \\\n\u001b[1;32m     19\u001b[0m                \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(W0))])\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from hypnettorch.hnets import HMLP\n",
        "hnet = HMLP(mnet.param_shapes, uncond_in_size=0, cond_in_size=8,\n",
        "            layers=[100, 100], num_cond_embs=2).to(device)\n",
        "\n",
        "print()\n",
        "print('The randomly initialized input embeddings are:\\n', \n",
        "      hnet.conditional_params)\n",
        "\n",
        "# To produce main network weights for condition `0`, we can either pass\n",
        "# the corresponding condition ID, or the corresponding (internally maintained)\n",
        "# embedding to the `forward` of the hypernetwork.\n",
        "W0 = hnet.forward(cond_id=0)\n",
        "W0_tmp = hnet.forward(cond_input=hnet.conditional_params[0].view(1, -1))\n",
        "assert np.all([torch.equal(W0[i], W0_tmp[i]) for i in range(len(W0))])\n",
        "\n",
        "# Hypernetworks also allow batch processing.\n",
        "W_batch = hnet.forward(cond_id=[0, 0])\n",
        "assert np.all([torch.equal(W_batch[0][i], W_batch[1][i]) \\\n",
        "               for i in range(len(W0))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "454rWdMWIY-f"
      },
      "outputs": [],
      "source": [
        "hnet.apply_hyperfan_init(mnet=mnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training.\n",
        "lr=1e-4\n",
        "batchsize=32\n",
        "nepochs=10\n",
        "\n",
        "# Adam usually works well in combination with hypernetwork training.\n",
        "optimizer = torch.optim.Adam(hnet.internal_params, lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
